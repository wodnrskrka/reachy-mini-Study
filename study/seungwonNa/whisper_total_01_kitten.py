import os
import sys
import tempfile
import requests
import speech_recognition as sr
import contextlib
import pygame
import noisereduce as nr
import soundfile as sf
import torch, cv2
import time

from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
from ultralytics import YOLO
from kittentts import KittenTTS

# ========== ì„¤ì • ==========
USE_GPU = torch.cuda.is_available()
DEVICE = 0 if USE_GPU else -1
WAKE_WORDS = ["í—¬ë¡œ", "hello", "hi", "í•˜ì´", "ì•ˆë…•"]
EXIT_WORDS = ["ì—†ì–´", "ëì–´", "ì•„ë‹ˆ", "nothing"]
MUSIC_PATH = "/home/naseungwon/reachy_mini/no-copyright-music-1.mp3"

# [ì´ˆê¸°í™”: YOLO ëª¨ë¸ ë¡œë“œ]
EMOTION_MODEL_PATH = "/home/naseungwon/reachy_mini/yolo_detect/best.pt"
emotion_model = YOLO(EMOTION_MODEL_PATH)
emotion_labels = ['anger', 'fear', 'happy', 'neutral', 'sad']

# ========== ì´ˆê¸°í™” ==========
pygame.mixer.init()

print("ğŸ§  Whisper-large-v3 model loading...")
stt_pipeline = pipeline("automatic-speech-recognition", model="openai/whisper-large-v3", device=DEVICE)

model_id = "google/flan-t5-base"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForSeq2SeqLM.from_pretrained(model_id)
llm = pipeline("text2text-generation", model=model, tokenizer=tokenizer)

# ========== Kitten TTS ì´ˆê¸°í™” ==========
try:
    # 24kHz ì¶œë ¥ì´ ê¸°ë³¸
    pygame.mixer.quit()
    pygame.mixer.init(frequency=24000)
except Exception:
    # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì¬ì´ˆê¸°í™”
    pygame.mixer.init()

KITTEN_SR = 24000
VOICE = 'expr-voice-2-f'  # ì‚¬ìš© ê°€ëŠ¥í•œ ë³´ì´ìŠ¤: 
# ['expr-voice-2-m','expr-voice-2-f','expr-voice-3-m','expr-voice-3-f',
#  'expr-voice-4-m','expr-voice-4-f','expr-voice-5-m','expr-voice-5-f']  :contentReference[oaicite:3]{index=3}

ktts = KittenTTS("KittenML/kitten-tts-nano-0.1")  # í”„ë¦¬ë·° ëª¨ë¸ ê²½ë¡œ  :contentReference[oaicite:4]{index=4}


# ========== ìœ í‹¸ í•¨ìˆ˜ ==========
def suppress_stderr_globally():
    sys.stderr = open(os.devnull, 'w')

@contextlib.contextmanager
def suppress_stderr():
    with open(os.devnull, 'w') as fnull:
        stderr = sys.stderr
        sys.stderr = fnull
        try:
            yield
        finally:
            sys.stderr = stderr

# ========== ìŒì„± ì…ë ¥ ==========
def listen_audio(timeout=10, phrase_time_limit=5, filename="input.wav"):
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("ğŸ¤ listening your voice...")
        with suppress_stderr():
            audio = recognizer.listen(source, timeout=timeout, phrase_time_limit=phrase_time_limit)
            with open(filename, "wb") as f:
                f.write(audio.get_wav_data())
            print(f"âœ… save complete: {filename}")
    return filename

def clean_audio(input_path="input.wav", output_path="cleaned.wav"):
    try:
        data, rate = sf.read(input_path)
        reduced = nr.reduce_noise(y=data, sr=rate)
        sf.write(output_path, reduced, rate)
        print(f"ğŸ”‡ killing noise complete â†’ {output_path}")
        return output_path
    except Exception as e:
        print("âŒ killing noise fail:", e)
        return input_path

def transcribe_audio(filename="cleaned.wav"):
    result = stt_pipeline(filename)
    print("ğŸ“ recognized text:", result['text'])
    return result['text'].strip()

# ========== í…ìŠ¤íŠ¸ ì‘ë‹µ ìƒì„± ==========
def generate_response(text):
    prompt = f"ì§ˆë¬¸: {text.strip()}\nëŒ€ë‹µ:"
    result = llm(prompt, max_new_tokens=100)
    response = result[0]["generated_text"].strip()
    return response if response else "sorry, I can't understand."

# ========== ìŒì„± ì¶œë ¥ ==========
def speak(text):
    text = (text or "").strip()
    if not text:
        print("âš ï¸ there is no text to spend.")
        return
    print(f"ğŸ—£ response: {text}")
    audio = ktts.generate(text, voice=VOICE)

    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as fp:
        sf.write(fp.name, audio, KITTEN_SR)
        pygame.mixer.music.load(fp.name)
        pygame.mixer.music.play()
        while pygame.mixer.music.get_busy():
            pass

# ========== ìŒì•… ì¬ìƒ ==========
music_state = "stopped"  # ìƒíƒœ: "playing", "paused", "stopped"

def play_music():
    if os.path.exists(MUSIC_PATH):
        pygame.mixer.music.load(MUSIC_PATH)
        pygame.mixer.music.play()
        print("ğŸµ playing music...")
    else:
        speak("Sorry, I can't find music file.")

def stop_music():
    if pygame.mixer.music.get_busy():
        pygame.mixer.music.stop()
        print("â¹ Music  off")

def speak_nonblocking(text, lang='ko'):
    text = (text or "").strip()
    if not text:
        return
    audio = ktts.generate(text, voice=VOICE)
    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as fp:
        sf.write(fp.name, audio, KITTEN_SR)
        sound = pygame.mixer.Sound(fp.name)
        ch = pygame.mixer.find_channel()
        if ch:
            ch.play(sound)

# ========== ë‚ ì”¨ ì •ë³´ ==========
def get_weather(city="Seoul"):
    API_KEY = "5d17af980c9cf48721b0e57c1b4fedaf"
    url = f"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={API_KEY}&lang=kr&units=metric"
    res = requests.get(url)
    if res.status_code == 200:
        data = res.json()
        desc = data["weather"][0]["description"]
        temp = data["main"]["temp"]
        return f"The current weather in {city} is {desc}, and the temperature is {temp:.1f}degrees."
    else:
        return "Failed to get weather information  ."

# ê°ì • ì¸ì‹ í•¨ìˆ˜ ì •ì˜
def detect_emotion_yolo(timeout=5):
    try:
        cap = cv2.VideoCapture(0)
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

        if not cap.isOpened():
            print("âŒ Unable to open your webcam.")
            return None

        print("ğŸ§  recognizing your motion ...")
        start_time = time.time()
        detected_emotion = None

        while time.time() - start_time < timeout:
            ret, frame = cap.read()
            if not ret:
                continue

            results = emotion_model.predict(frame, imgsz=640, verbose=False)[0]
            if len(results.boxes) > 0:
                # ê°€ì¥ í° ì–¼êµ´ ê¸°ì¤€ (ì˜ˆì™¸ ë°©ì§€ìš© if ë¬¸ ë³´ê°•)
                try:
                    box = max(results.boxes, key=lambda b: (b.xyxy[0][2] - b.xyxy[0][0]) * (b.xyxy[0][3] - b.xyxy[0][1]))
                    class_id = int(box.cls.item())
                    detected_emotion = emotion_labels[class_id]

                    # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
                    xyxy = box.xyxy[0].cpu().numpy().astype(int)
                    cv2.rectangle(frame, (xyxy[0], xyxy[1]), (xyxy[2], xyxy[3]), (0, 255, 0), 2)
                    cv2.putText(frame, detected_emotion, (xyxy[0], xyxy[1] - 10),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)

                    cv2.imshow("Emotion Detection", frame)
                    cv2.waitKey(1000)
                    break
                except Exception as e:
                    print(f"â—Error occurs during emotion recognition: {e}")
                    continue

            # ì–¼êµ´ ì¸ì‹ ì•ˆ ë˜ì—ˆì„ ê²½ìš°
            cv2.imshow("Emotion Detection", frame)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

        cap.release()
        cv2.destroyAllWindows()

        return detected_emotion

    except Exception as e:
        print(f"ğŸš¨ Exception occurs during emotion recognition: {e}")
        return None

# ========== ë©”ì¸ ë£¨í”„ ==========
def main():
    suppress_stderr_globally()
    print("ğŸ¤– 'Waiting for Reachy Mini ...")

    # [1] ì›¨ì´í¬ ì›Œë“œ ëŒ€ê¸°
    while True:
        path = listen_audio()
        cleaned = clean_audio(path)
        transcript = transcribe_audio(cleaned).lower()
        if any(transcript.startswith(wake) or transcript == wake for wake in WAKE_WORDS):
            speak("Hello! What can I help you?")
            break

    # [2] ëª…ë ¹ ì²˜ë¦¬ ë£¨í”„ (ì§€ì†ì ìœ¼ë¡œ ëª…ë ¹ ìˆ˜ìš©)
    while True:
        path = listen_audio()
        cleaned = clean_audio(path)
        user_text = transcribe_audio(cleaned).lower()

        if any(bye in user_text for bye in EXIT_WORDS):
            speak("Exit the program.")
            break

        # ëª…ë ¹ ì²˜ë¦¬
        if "weather" in user_text:
            response = get_weather()
            speak(response)

        # ê°ì • ë¶„ì„ ë¶„ê¸° ì¶”ê°€
        elif "feeling" in user_text or "my feeling" in user_text:
            speak("I'll see how it feels. Please look at the camera for a moment")
            emotion = detect_emotion_yolo()
            if emotion in ['happy', 'sad']:
                if emotion == 'happy':
                    speak(f"You look happy! ({emotion})")
                elif emotion == 'sad':
                    speak(f"You look sad. What's happening? ({emotion})")
            else:
                speak("Sorry, I can't understand you emotion.")

        elif "music" in user_text or "sing" in user_text:
            speak("I'll play the music.")
            play_music()

            # ìŒì•… ì¬ìƒ ì¤‘ ë©ˆì¶¤ ê°ì§€ ë£¨í”„
            while True:
                path = listen_audio()
                cleaned = clean_audio(path)
                cmd = transcribe_audio(cleaned).lower()
                if "turn off" or "stop" in cmd:
                    stop_music()
                    speak("I'll turn off the music.")
                    break
                else:
                    speak_nonblocking("Sorry, I didn't understand.")
                    # ìŒì•… ê³„ì† ìœ ì§€ (ì¤‘ë‹¨ ì•ˆ ë¨!)

        else:
            response = generate_response(user_text)
            speak(response)

        # ë‹¤ìŒ ì§ˆë¬¸ ìœ ë„
        speak("Is there anything I can help you?")


if __name__ == "__main__":
    main()
